"""
å¸Œåˆ©å‰µæ–°å¨ƒå¨ƒæ©Ÿ äº¤æ˜“åˆ†æå„€è¡¨æ¿
è³‡æ–™ä¾†æºï¼šSEGA_TX/å¸Œåˆ©å‰µæ–°/ URS-YYYY-MM-DD.csv
- è‡ªå‹•åŒ–ï¼šè‹¥åµæ¸¬åˆ°è³‡æ–™ç›®éŒ„ä¸”æœ‰ URS-*.csvï¼Œé–‹å•Ÿé é¢æ™‚è‡ªå‹•è¼‰å…¥ï¼Œç„¡éœ€æ‰‹å‹•æŒ‰éˆ•ã€‚
"""
import os
import glob
import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime

try:
    from config import BASE_DIR
except ImportError:
    BASE_DIR = os.path.abspath(os.getcwd())

# å¸Œåˆ©å‰µæ–°æ¯æ—¥å ±è¡¨ç›®éŒ„ï¼šå˜—è©¦å¤šç¨®ç›¸å°è·¯å¾‘ï¼ˆæœ¬æ©Ÿ Web èˆ‡ SEGA_TX åŒå±¤ï¼›é›²ç«¯å¯èƒ½ç‚º src/Web ä¸” SEGA_TX åœ¨ repo æ ¹ï¼‰
def _resolve_hili_data_dir():
    candidates = [
        os.path.join(BASE_DIR, "..", "SEGA_TX", "å¸Œåˆ©å‰µæ–°"),
        os.path.join(BASE_DIR, "..", "..", "SEGA_TX", "å¸Œåˆ©å‰µæ–°"),
    ]
    for p in candidates:
        resolved = os.path.abspath(p)
        if os.path.isdir(resolved):
            return resolved
    return os.path.abspath(candidates[0])

HILI_DATA_DIR = _resolve_hili_data_dir()
REQUIRED_COLS = ["å•†åº—åç¨±", "äº¤æ˜“æ—¥æœŸ", "æ˜¯å¦é€€æ¬¾", "ç™¼å¡å…¬å¸", "å¯¦éš›æ‰£æ¬¾é‡‘é¡"]


def _store_short_name(full_name):
    if pd.isna(full_name) or not isinstance(full_name, str):
        return full_name
    s = full_name.strip()
    return s.split()[-1] if " " in s else s


def load_urs_csv(path_or_file, encoding="utf-8"):
    if hasattr(path_or_file, "read"):
        df = pd.read_csv(path_or_file, encoding=encoding)
    else:
        df = pd.read_csv(path_or_file, encoding=encoding)
    if not all(c in df.columns for c in REQUIRED_COLS):
        return None
    df = df[df["æ˜¯å¦é€€æ¬¾"].astype(str).str.strip() != "æ˜¯"].copy()
    df["äº¤æ˜“æ—¥æœŸ"] = pd.to_numeric(df["äº¤æ˜“æ—¥æœŸ"], errors="coerce").astype("Int64")
    df = df.dropna(subset=["äº¤æ˜“æ—¥æœŸ"])
    df["æ—¥æœŸ"] = pd.to_datetime(df["äº¤æ˜“æ—¥æœŸ"].astype(str), format="%Y%m%d", errors="coerce")
    df = df.dropna(subset=["æ—¥æœŸ"])
    df["åº—å"] = df["å•†åº—åç¨±"].map(_store_short_name)
    df["æ”¯ä»˜åˆ¥"] = df["ç™¼å¡å…¬å¸"].fillna("ç¾é‡‘").astype(str).str.strip().replace("", "ç¾é‡‘")
    df["é‡‘é¡"] = pd.to_numeric(df["å¯¦éš›æ‰£æ¬¾é‡‘é¡"], errors="coerce").fillna(0)
    return df[["åº—å", "æ—¥æœŸ", "æ”¯ä»˜åˆ¥", "é‡‘é¡", "äº¤æ˜“æ—¥æœŸ"]]


def load_hili_data(data_dir=None, uploaded_files=None):
    if uploaded_files:
        dfs = []
        for f in uploaded_files:
            if f.name.lower().endswith(".csv"):
                try:
                    df = load_urs_csv(f)
                    if df is not None:
                        dfs.append(df)
                except Exception as e:
                    return None, f"è®€å– {f.name} å¤±æ•—ï¼š{e}"
        if not dfs:
            return None, "æ²’æœ‰å¯ç”¨çš„ URS æ ¼å¼ CSV"
        return pd.concat(dfs, ignore_index=True), None

    if not data_dir or not os.path.isdir(data_dir):
        return None, None
    files = sorted(glob.glob(os.path.join(data_dir, "URS-*.csv")))
    if not files:
        return None, f"ç›®éŒ„å…§æ²’æœ‰ URS-*.csvï¼š{data_dir}"
    dfs = []
    for path in files:
        try:
            df = load_urs_csv(path)
            if df is not None:
                dfs.append(df)
        except Exception as e:
            return None, f"è®€å– {os.path.basename(path)} å¤±æ•—ï¼š{e}"
    if not dfs:
        return None, "æ²’æœ‰ç¬¦åˆæ¬„ä½æ ¼å¼çš„æª”æ¡ˆ"
    return pd.concat(dfs, ignore_index=True), None


def build_summary_table(df, date_min, date_max):
    if df.empty:
        return pd.DataFrame()
    days = max(1, (date_max - date_min).days + 1)
    g = df.groupby("åº—å").agg(ç´¯ç©ç‡Ÿæ”¶=("é‡‘é¡", "sum")).reset_index()
    g["å¹³å‡æ—¥ç‡Ÿæ”¶"] = (g["ç´¯ç©ç‡Ÿæ”¶"] / days).round(0)
    g["é ä¼°æœˆç‡Ÿæ”¶"] = (g["å¹³å‡æ—¥ç‡Ÿæ”¶"] * 30).round(0)
    g["é ä¼°å¹´ç‡Ÿæ”¶"] = (g["å¹³å‡æ—¥ç‡Ÿæ”¶"] * 365).round(0)
    cols = ["åº—å", "ç´¯ç©ç‡Ÿæ”¶", "å¹³å‡æ—¥ç‡Ÿæ”¶", "é ä¼°æœˆç‡Ÿæ”¶", "é ä¼°å¹´ç‡Ÿæ”¶"]
    total_row = {"åº—å": "ç¸½è¨ˆ", "ç´¯ç©ç‡Ÿæ”¶": g["ç´¯ç©ç‡Ÿæ”¶"].sum(), "å¹³å‡æ—¥ç‡Ÿæ”¶": g["å¹³å‡æ—¥ç‡Ÿæ”¶"].sum(),
                 "é ä¼°æœˆç‡Ÿæ”¶": g["é ä¼°æœˆç‡Ÿæ”¶"].sum(), "é ä¼°å¹´ç‡Ÿæ”¶": g["é ä¼°å¹´ç‡Ÿæ”¶"].sum()}
    g = pd.concat([g, pd.DataFrame([total_row])], ignore_index=True)
    store_only = g[g["åº—å"] != "ç¸½è¨ˆ"]
    avg_row = {"åº—å": "å¹³å‡å€¼", "ç´¯ç©ç‡Ÿæ”¶": store_only["ç´¯ç©ç‡Ÿæ”¶"].mean().round(0),
               "å¹³å‡æ—¥ç‡Ÿæ”¶": store_only["å¹³å‡æ—¥ç‡Ÿæ”¶"].mean().round(0),
               "é ä¼°æœˆç‡Ÿæ”¶": store_only["é ä¼°æœˆç‡Ÿæ”¶"].mean().round(0),
               "é ä¼°å¹´ç‡Ÿæ”¶": store_only["é ä¼°å¹´ç‡Ÿæ”¶"].mean().round(0)}
    g = pd.concat([g, pd.DataFrame([avg_row])], ignore_index=True)
    operating = store_only[store_only["ç´¯ç©ç‡Ÿæ”¶"] > 0]
    if not operating.empty:
        avg_op_row = {"åº—å": "å¹³å‡å€¼ï¼ˆç‡Ÿæ¥­ä¸­ï¼‰", "ç´¯ç©ç‡Ÿæ”¶": operating["ç´¯ç©ç‡Ÿæ”¶"].mean().round(0),
                      "å¹³å‡æ—¥ç‡Ÿæ”¶": operating["å¹³å‡æ—¥ç‡Ÿæ”¶"].mean().round(0),
                      "é ä¼°æœˆç‡Ÿæ”¶": operating["é ä¼°æœˆç‡Ÿæ”¶"].mean().round(0),
                      "é ä¼°å¹´ç‡Ÿæ”¶": operating["é ä¼°å¹´ç‡Ÿæ”¶"].mean().round(0)}
        g = pd.concat([g, pd.DataFrame([avg_op_row])], ignore_index=True)
    return g[cols]


# --- é é¢ ---
st.set_page_config(page_title="å¸Œåˆ©å‰µæ–° äº¤æ˜“åˆ†æå„€è¡¨æ¿", page_icon="ğŸ“Š", layout="wide", initial_sidebar_state="expanded")
st.markdown("## å¸Œåˆ©å‰µæ–°å¨ƒå¨ƒæ©Ÿ äº¤æ˜“åˆ†æå„€è¡¨æ¿")

# è‡ªå‹•åŒ–ï¼šè‹¥å°šç„¡è³‡æ–™ä¸”è³‡æ–™ç›®éŒ„å­˜åœ¨ä¸”æœ‰ URS-*.csvï¼Œè‡ªå‹•è¼‰å…¥
if "hili_raw_df" not in st.session_state or st.session_state["hili_raw_df"] is None:
    if os.path.isdir(HILI_DATA_DIR):
        files = glob.glob(os.path.join(HILI_DATA_DIR, "URS-*.csv"))
        if files:
            raw_df, err = load_hili_data(data_dir=HILI_DATA_DIR)
            if raw_df is not None and err is None:
                st.session_state["hili_raw_df"] = raw_df

# å´é‚Šæ¬„ï¼šè³‡æ–™ä¾†æºèˆ‡æ‰‹å‹•é¸é …
with st.sidebar:
    st.subheader("è³‡æ–™ä¾†æº")
    use_dir = os.path.isdir(HILI_DATA_DIR)
    if use_dir:
        st.caption(f"ç›®éŒ„ï¼š{HILI_DATA_DIR}")
        if st.button("é‡æ–°å¾ç›®éŒ„è¼‰å…¥"):
            raw_df, err = load_hili_data(data_dir=HILI_DATA_DIR)
            if raw_df is not None:
                st.session_state["hili_raw_df"] = raw_df
                st.success(f"å·²è¼‰å…¥ {len(raw_df):,} ç­†")
            elif err:
                st.error(err)
        source = st.radio("æˆ–", ["ä½¿ç”¨å·²è¼‰å…¥è³‡æ–™", "ä¸Šå‚³ CSV"], key="hili_src")
    else:
        st.caption(f"æœªåµæ¸¬åˆ°ç›®éŒ„ï¼š{HILI_DATA_DIR}")
        source = "ä¸Šå‚³ CSV"

    if source == "ä¸Šå‚³ CSV":
        uploaded = st.file_uploader("ä¸Šå‚³ URS-*.csvï¼ˆå¯å¤šæª”ï¼‰", type=["csv"], accept_multiple_files=True)
        if uploaded:
            raw_df, err = load_hili_data(uploaded_files=uploaded)
            if raw_df is not None:
                st.session_state["hili_raw_df"] = raw_df
                st.success(f"å·²è¼‰å…¥ {len(raw_df):,} ç­†")
            elif err:
                st.error(err)

if "hili_raw_df" not in st.session_state or st.session_state["hili_raw_df"] is None:
    st.info("è«‹åœ¨å·¦å´ã€Œä¸Šå‚³ CSVã€æˆ–ç¢ºèªè³‡æ–™ç›®éŒ„å­˜åœ¨ä¸¦æŒ‰ã€Œé‡æ–°å¾ç›®éŒ„è¼‰å…¥ã€ã€‚")
    st.stop()

df = st.session_state["hili_raw_df"]
date_min_all, date_max_all = df["æ—¥æœŸ"].min(), df["æ—¥æœŸ"].max()

st.subheader("äº¤æ˜“æ—¥æœŸç¯©é¸")
col_y1, col_m1, col_d1, col_y2, col_m2, col_d2 = st.columns(6)
with col_y1:
    y1 = st.number_input("èµ·å§‹å¹´", value=date_min_all.year, min_value=date_min_all.year, max_value=date_max_all.year, key="y1")
with col_m1:
    m1 = st.number_input("èµ·å§‹æœˆ", value=date_min_all.month, min_value=1, max_value=12, key="m1")
with col_d1:
    d1 = st.number_input("èµ·å§‹æ—¥", value=date_min_all.day, min_value=1, max_value=31, key="d1")
with col_y2:
    y2 = st.number_input("çµæŸå¹´", value=date_max_all.year, min_value=date_min_all.year, max_value=date_max_all.year, key="y2")
with col_m2:
    m2 = st.number_input("çµæŸæœˆ", value=date_max_all.month, min_value=1, max_value=12, key="m2")
with col_d2:
    d2 = st.number_input("çµæŸæ—¥", value=date_max_all.day, min_value=1, max_value=31, key="d2")

try:
    filter_start = datetime(y1, m1, d1).date()
    filter_end = datetime(y2, m2, d2).date()
except ValueError:
    filter_start, filter_end = date_min_all.date(), date_max_all.date()

df_f = df[(df["æ—¥æœŸ"].dt.date >= filter_start) & (df["æ—¥æœŸ"].dt.date <= filter_end)].copy()
if df_f.empty:
    st.warning("ç¯©é¸å€é–“å…§ç„¡è³‡æ–™ã€‚")
    st.stop()

stat_days = max(1, (filter_end - filter_start).days + 1)
last_date = df_f["æ—¥æœŸ"].max().strftime("%Y-%m-%d")
store_count = df_f["åº—å"].nunique()
summary = build_summary_table(df_f, pd.Timestamp(filter_start), pd.Timestamp(filter_end))

st.subheader("è³‡æ–™é‡æ•´")
c1, c2, c3 = st.columns(3)
c1.metric("æœ€å¾Œäº¤æ˜“æ—¥", last_date)
c2.metric("çµ±è¨ˆæ—¥æ•¸", stat_days)
c3.metric("ç‡Ÿé‹é€šè·¯æ•¸", store_count)
st.dataframe(summary, use_container_width=True, hide_index=True)

left, right = st.columns(2)
with left:
    st.subheader("å„æ”¯ä»˜åˆ¥äº¤æ˜“é‡‘é¡æ¯”")
    pay_agg = df_f.groupby("æ”¯ä»˜åˆ¥")["é‡‘é¡"].sum().reset_index(name="é‡‘é¡")
    if not pay_agg.empty:
        st.plotly_chart(px.pie(pay_agg, values="é‡‘é¡", names="æ”¯ä»˜åˆ¥"), use_container_width=True)
with right:
    st.subheader("å„é–€å¸‚äº¤æ˜“ç¸½é‡‘é¡æ¯”è¼ƒ")
    store_options = sorted(df_f["åº—å"].unique())
    selected_stores = st.multiselect("é–€å¸‚ç¯©é¸", store_options, default=store_options, key="store_filter")
    df_store = df_f[df_f["åº—å"].isin(selected_stores)].groupby("åº—å")["é‡‘é¡"].sum().sort_values(ascending=True)
    if not df_store.empty:
        st.plotly_chart(px.bar(x=df_store.values, y=df_store.index, orientation="h", labels={"x": "äº¤æ˜“é‡‘é¡", "y": "é–€å¸‚"}), use_container_width=True)

st.subheader("å¸Œåˆ©å‰µæ–°å¨ƒå¨ƒæ©Ÿæ¯æ—¥äº¤æ˜“é‡‘é¡")
daily = df_f.groupby("æ—¥æœŸ")["é‡‘é¡"].sum().reset_index()
st.plotly_chart(px.line(daily, x="æ—¥æœŸ", y="é‡‘é¡"), use_container_width=True)

st.subheader("å„æ”¯ä»˜åˆ¥é‡‘é¡å æ¯”ï¼ˆä¾æ—¥æœŸï¼‰")
pay_daily = df_f.pivot_table(index="æ—¥æœŸ", columns="æ”¯ä»˜åˆ¥", values="é‡‘é¡", aggfunc="sum", fill_value=0)
pay_daily_pct = pay_daily.div(pay_daily.sum(axis=1), axis=0).fillna(0) * 100
pay_daily_pct = pay_daily_pct.reset_index()
fig_stack = go.Figure()
for col in pay_daily_pct.columns:
    if col == "æ—¥æœŸ":
        continue
    fig_stack.add_trace(go.Scatter(x=pay_daily_pct["æ—¥æœŸ"], y=pay_daily_pct[col], name=col, stackgroup="one", mode="lines"))
fig_stack.update_layout(yaxis_title="å æ¯” (%)", xaxis_title="æ—¥æœŸ", hovermode="x unified")
st.plotly_chart(fig_stack, use_container_width=True)

st.subheader("å–®ä¸€é–€å¸‚ç´°éƒ¨")
store_detail = st.selectbox("é¸æ“‡é–€å¸‚", ["ï¼ˆå…¨éƒ¨ï¼‰"] + sorted(df_f["åº—å"].unique()), key="store_detail")
if store_detail != "ï¼ˆå…¨éƒ¨ï¼‰":
    df_s = df_f[df_f["åº—å"] == store_detail]
    r1, r2 = st.columns(2)
    with r1:
        st.markdown(f"**{store_detail} æ¯æ—¥äº¤æ˜“é‡‘é¡**")
        st.plotly_chart(px.line(df_s.groupby("æ—¥æœŸ")["é‡‘é¡"].sum().reset_index(), x="æ—¥æœŸ", y="é‡‘é¡"), use_container_width=True)
    with r2:
        st.markdown(f"**{store_detail} å„æ”¯ä»˜åˆ¥é‡‘é¡**")
        st.plotly_chart(px.bar(df_s.groupby("æ”¯ä»˜åˆ¥")["é‡‘é¡"].sum().reset_index(), x="æ”¯ä»˜åˆ¥", y="é‡‘é¡"), use_container_width=True)
