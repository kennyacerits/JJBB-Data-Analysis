"""
å¸Œåˆ©å‰µæ–°å¨ƒå¨ƒæ©Ÿ äº¤æ˜“åˆ†æå„€è¡¨æ¿
è³‡æ–™ä¾†æºï¼š../æ•¸æ“šåˆ†æ/BI_å¸Œåˆ©å‰µæ–°/output/ æˆ– ../æ•¸æ“šåˆ†æ/å¸Œåˆ©å‰µæ–°/output/ ä¹‹ã€Œå¸Œåˆ©å‰µæ–°å¨ƒå¨ƒæ©Ÿ_æ¯æ—¥äº¤æ˜“æ˜ç´°.csvã€ï¼ˆå·²å½™ç¸½å„é–€å¸‚å„æ”¯ä»˜åˆ¥ç­†æ•¸èˆ‡é‡‘é¡ï¼‰
"""
import os
import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime

try:
    from config import BASE_DIR
except ImportError:
    BASE_DIR = os.path.abspath(os.getcwd())

# å½™ç¸½æª”æª”å
HILI_SUMMARY_FILENAME = "å¸Œåˆ©å‰µæ–°å¨ƒå¨ƒæ©Ÿ_æ¯æ—¥äº¤æ˜“æ˜ç´°.csv"
# å½™ç¸½æª”å¿…è¦æ¬„ä½ï¼ˆå°æ‡‰å„€è¡¨æ¿ï¼šåº—åã€æ—¥æœŸã€æ”¯ä»˜åˆ¥ã€é‡‘é¡ï¼‰
REQUIRED_COLS = ["äº¤æ˜“æ—¥æœŸ", "å•†åº—é€šç¨±", "æ”¯ä»˜å·¥å…·", "é‡‘é¡"]


def _resolve_hili_summary_path():
    """è§£æå½™ç¸½æª”è·¯å¾‘ï¼šç’°å¢ƒè®Šæ•¸ > æ•¸æ“šåˆ†æ/BI_å¸Œåˆ©å‰µæ–°/output > æ•¸æ“šåˆ†æ/å¸Œåˆ©å‰µæ–°/output > å°ˆæ¡ˆå…§"""
    env_file = os.environ.get("HILI_DATA_FILE")
    if env_file:
        p = os.path.abspath(os.path.expanduser(env_file))
        if os.path.isfile(p):
            return p
    candidates = [
        os.path.join(BASE_DIR, "..", "æ•¸æ“šåˆ†æ", "BI_å¸Œåˆ©å‰µæ–°", "output", HILI_SUMMARY_FILENAME),
        os.path.join(BASE_DIR, "..", "æ•¸æ“šåˆ†æ", "å¸Œåˆ©å‰µæ–°", "output", HILI_SUMMARY_FILENAME),
        os.path.join(BASE_DIR, "..", "..", "æ•¸æ“šåˆ†æ", "BI_å¸Œåˆ©å‰µæ–°", "output", HILI_SUMMARY_FILENAME),
        os.path.join(BASE_DIR, "..", "..", "æ•¸æ“šåˆ†æ", "å¸Œåˆ©å‰µæ–°", "output", HILI_SUMMARY_FILENAME),
        os.path.join(BASE_DIR, "æ•¸æ“šåˆ†æ", "å¸Œåˆ©å‰µæ–°", "output", HILI_SUMMARY_FILENAME),
        os.path.join(BASE_DIR, "æ•¸æ“šåˆ†æ", "BI_å¸Œåˆ©å‰µæ–°", "output", HILI_SUMMARY_FILENAME),
    ]
    for p in candidates:
        resolved = os.path.abspath(p)
        if os.path.isfile(resolved):
            return resolved
    return os.path.abspath(candidates[0])


def load_summary_csv(path_or_file, encoding="utf-8"):
    """è¼‰å…¥æ¯æ—¥äº¤æ˜“æ˜ç´°å½™ç¸½æª”ï¼Œå›å‚³æ¨™æº–åŒ– DataFrameï¼ˆåº—åã€æ—¥æœŸã€æ”¯ä»˜åˆ¥ã€é‡‘é¡ï¼‰ã€‚"""
    if hasattr(path_or_file, "read"):
        df = pd.read_csv(path_or_file, encoding=encoding)
    else:
        df = pd.read_csv(path_or_file, encoding=encoding)
    if not all(c in df.columns for c in REQUIRED_COLS):
        return None, f"ç¼ºå°‘æ¬„ä½ï¼Œéœ€å«ï¼š{', '.join(REQUIRED_COLS)}"
    df = df.copy()
    df["æ—¥æœŸ"] = pd.to_datetime(df["äº¤æ˜“æ—¥æœŸ"], errors="coerce")
    df = df.dropna(subset=["æ—¥æœŸ"])
    df["åº—å"] = df["å•†åº—é€šç¨±"].astype(str).str.strip()
    df["æ”¯ä»˜åˆ¥"] = df["æ”¯ä»˜å·¥å…·"].fillna("ç¾é‡‘").astype(str).str.strip().replace("", "ç¾é‡‘")
    df["é‡‘é¡"] = pd.to_numeric(df["é‡‘é¡"], errors="coerce").fillna(0)
    return df[["åº—å", "æ—¥æœŸ", "æ”¯ä»˜åˆ¥", "é‡‘é¡"]], None


def load_hili_data(file_path=None, uploaded_file=None):
    """è¼‰å…¥å¸Œåˆ©å‰µæ–°è³‡æ–™ã€‚file_path ç‚ºå½™ç¸½æª”è·¯å¾‘ï¼Œæˆ– uploaded_file ç‚ºä¸Šå‚³æª”æ¡ˆã€‚å›å‚³ (df, error_msg)ã€‚"""
    if uploaded_file is not None:
        return load_summary_csv(uploaded_file)
    if file_path and os.path.isfile(file_path):
        return load_summary_csv(file_path)
    return None, None


def build_summary_table(df, date_min, date_max):
    if df.empty:
        return pd.DataFrame()
    days = max(1, (date_max - date_min).days + 1)
    g = df.groupby("åº—å").agg(ç´¯ç©ç‡Ÿæ”¶=("é‡‘é¡", "sum")).reset_index()
    g["å¹³å‡æ—¥ç‡Ÿæ”¶"] = (g["ç´¯ç©ç‡Ÿæ”¶"] / days).round(0)
    g["é ä¼°æœˆç‡Ÿæ”¶"] = (g["å¹³å‡æ—¥ç‡Ÿæ”¶"] * 30).round(0)
    g["é ä¼°å¹´ç‡Ÿæ”¶"] = (g["å¹³å‡æ—¥ç‡Ÿæ”¶"] * 365).round(0)
    cols = ["åº—å", "ç´¯ç©ç‡Ÿæ”¶", "å¹³å‡æ—¥ç‡Ÿæ”¶", "é ä¼°æœˆç‡Ÿæ”¶", "é ä¼°å¹´ç‡Ÿæ”¶"]
    total_row = {"åº—å": "ç¸½è¨ˆ", "ç´¯ç©ç‡Ÿæ”¶": g["ç´¯ç©ç‡Ÿæ”¶"].sum(), "å¹³å‡æ—¥ç‡Ÿæ”¶": g["å¹³å‡æ—¥ç‡Ÿæ”¶"].sum(),
                 "é ä¼°æœˆç‡Ÿæ”¶": g["é ä¼°æœˆç‡Ÿæ”¶"].sum(), "é ä¼°å¹´ç‡Ÿæ”¶": g["é ä¼°å¹´ç‡Ÿæ”¶"].sum()}
    g = pd.concat([g, pd.DataFrame([total_row])], ignore_index=True)
    store_only = g[g["åº—å"] != "ç¸½è¨ˆ"]
    avg_row = {"åº—å": "å¹³å‡å€¼", "ç´¯ç©ç‡Ÿæ”¶": store_only["ç´¯ç©ç‡Ÿæ”¶"].mean().round(0),
               "å¹³å‡æ—¥ç‡Ÿæ”¶": store_only["å¹³å‡æ—¥ç‡Ÿæ”¶"].mean().round(0),
               "é ä¼°æœˆç‡Ÿæ”¶": store_only["é ä¼°æœˆç‡Ÿæ”¶"].mean().round(0),
               "é ä¼°å¹´ç‡Ÿæ”¶": store_only["é ä¼°å¹´ç‡Ÿæ”¶"].mean().round(0)}
    g = pd.concat([g, pd.DataFrame([avg_row])], ignore_index=True)
    operating = store_only[store_only["ç´¯ç©ç‡Ÿæ”¶"] > 0]
    if not operating.empty:
        avg_op_row = {"åº—å": "å¹³å‡å€¼ï¼ˆç‡Ÿæ¥­ä¸­ï¼‰", "ç´¯ç©ç‡Ÿæ”¶": operating["ç´¯ç©ç‡Ÿæ”¶"].mean().round(0),
                      "å¹³å‡æ—¥ç‡Ÿæ”¶": operating["å¹³å‡æ—¥ç‡Ÿæ”¶"].mean().round(0),
                      "é ä¼°æœˆç‡Ÿæ”¶": operating["é ä¼°æœˆç‡Ÿæ”¶"].mean().round(0),
                      "é ä¼°å¹´ç‡Ÿæ”¶": operating["é ä¼°å¹´ç‡Ÿæ”¶"].mean().round(0)}
        g = pd.concat([g, pd.DataFrame([avg_op_row])], ignore_index=True)
    return g[cols]


# --- é é¢ ---
st.set_page_config(page_title="å¸Œåˆ©å‰µæ–° äº¤æ˜“åˆ†æå„€è¡¨æ¿", page_icon="ğŸ“Š", layout="wide", initial_sidebar_state="expanded")
st.markdown("## å¸Œåˆ©å‰µæ–°å¨ƒå¨ƒæ©Ÿ äº¤æ˜“åˆ†æå„€è¡¨æ¿")

HILI_SUMMARY_PATH = _resolve_hili_summary_path()
file_exists = os.path.isfile(HILI_SUMMARY_PATH)

# è‡ªå‹•åŒ–ï¼šè‹¥å°šç„¡è³‡æ–™ä¸”å½™ç¸½æª”å­˜åœ¨ï¼Œè‡ªå‹•è¼‰å…¥
if "hili_raw_df" not in st.session_state or st.session_state["hili_raw_df"] is None:
    if file_exists:
        raw_df, err = load_hili_data(file_path=HILI_SUMMARY_PATH)
        if raw_df is not None and err is None:
            st.session_state["hili_raw_df"] = raw_df

# å´é‚Šæ¬„
with st.sidebar:
    st.subheader("è³‡æ–™ä¾†æº")
    if file_exists:
        st.caption(f"å·²åµæ¸¬åˆ°ï¼š{HILI_SUMMARY_FILENAME}")
        st.code(HILI_SUMMARY_PATH, language=None)
        if st.button("é‡æ–°å¾é è¨­è·¯å¾‘è¼‰å…¥"):
            raw_df, err = load_hili_data(file_path=HILI_SUMMARY_PATH)
            if raw_df is not None:
                st.session_state["hili_raw_df"] = raw_df
                st.success(f"å·²è¼‰å…¥ {len(raw_df):,} ç­†")
            elif err:
                st.error(err)
        source = st.radio("æˆ–", ["ä½¿ç”¨å·²è¼‰å…¥è³‡æ–™", "ä¸Šå‚³ CSV"], key="hili_src")
    else:
        st.caption("æœªåµæ¸¬åˆ°å½™ç¸½æª”ã€‚è«‹åœ¨å°ˆæ¡ˆå…§æˆ–ä¸Šä¸€å±¤å»ºç«‹ æ•¸æ“šåˆ†æ/BI_å¸Œåˆ©å‰µæ–°/output/ï¼ˆæˆ– æ•¸æ“šåˆ†æ/å¸Œåˆ©å‰µæ–°/output/ï¼‰ä¸¦æ”¾å…¥ã€Œå¸Œåˆ©å‰µæ–°å¨ƒå¨ƒæ©Ÿ_æ¯æ—¥äº¤æ˜“æ˜ç´°.csvã€ï¼Œæˆ–ä½¿ç”¨ã€Œä¸Šå‚³ CSVã€ã€‚")
        st.code(HILI_SUMMARY_PATH, language=None)
        source = "ä¸Šå‚³ CSV"

    if source == "ä¸Šå‚³ CSV":
        uploaded = st.file_uploader(f"ä¸Šå‚³ {HILI_SUMMARY_FILENAME}ï¼ˆæˆ–åŒæ ¼å¼ï¼‰", type=["csv"])
        if uploaded:
            raw_df, err = load_hili_data(uploaded_file=uploaded)
            if raw_df is not None:
                st.session_state["hili_raw_df"] = raw_df
                st.success(f"å·²è¼‰å…¥ {len(raw_df):,} ç­†")
            elif err:
                st.error(err)

if "hili_raw_df" not in st.session_state or st.session_state["hili_raw_df"] is None:
    st.info("è«‹åœ¨å·¦å´ä¸Šå‚³ã€Œå¸Œåˆ©å‰µæ–°å¨ƒå¨ƒæ©Ÿ_æ¯æ—¥äº¤æ˜“æ˜ç´°.csvã€ï¼Œæˆ–ç¢ºèªé è¨­è·¯å¾‘å­˜åœ¨è©²æª”ä¸¦æŒ‰ã€Œé‡æ–°å¾é è¨­è·¯å¾‘è¼‰å…¥ã€ã€‚")
    st.stop()

df = st.session_state["hili_raw_df"]
date_min_all, date_max_all = df["æ—¥æœŸ"].min(), df["æ—¥æœŸ"].max()

st.subheader("äº¤æ˜“æ—¥æœŸç¯©é¸")
col_y1, col_m1, col_d1, col_y2, col_m2, col_d2 = st.columns(6)
with col_y1:
    y1 = st.number_input("èµ·å§‹å¹´", value=date_min_all.year, min_value=date_min_all.year, max_value=date_max_all.year, key="y1")
with col_m1:
    m1 = st.number_input("èµ·å§‹æœˆ", value=date_min_all.month, min_value=1, max_value=12, key="m1")
with col_d1:
    d1 = st.number_input("èµ·å§‹æ—¥", value=date_min_all.day, min_value=1, max_value=31, key="d1")
with col_y2:
    y2 = st.number_input("çµæŸå¹´", value=date_max_all.year, min_value=date_min_all.year, max_value=date_max_all.year, key="y2")
with col_m2:
    m2 = st.number_input("çµæŸæœˆ", value=date_max_all.month, min_value=1, max_value=12, key="m2")
with col_d2:
    d2 = st.number_input("çµæŸæ—¥", value=date_max_all.day, min_value=1, max_value=31, key="d2")

try:
    filter_start = datetime(y1, m1, d1).date()
    filter_end = datetime(y2, m2, d2).date()
except ValueError:
    filter_start, filter_end = date_min_all.date(), date_max_all.date()

df_f = df[(df["æ—¥æœŸ"].dt.date >= filter_start) & (df["æ—¥æœŸ"].dt.date <= filter_end)].copy()
if df_f.empty:
    st.warning("ç¯©é¸å€é–“å…§ç„¡è³‡æ–™ã€‚")
    st.stop()

stat_days = max(1, (filter_end - filter_start).days + 1)
last_date = df_f["æ—¥æœŸ"].max().strftime("%Y-%m-%d")
store_count = df_f["åº—å"].nunique()
summary = build_summary_table(df_f, pd.Timestamp(filter_start), pd.Timestamp(filter_end))

st.subheader("è³‡æ–™é‡æ•´")
c1, c2, c3 = st.columns(3)
c1.metric("æœ€å¾Œäº¤æ˜“æ—¥", last_date)
c2.metric("çµ±è¨ˆæ—¥æ•¸", stat_days)
c3.metric("ç‡Ÿé‹é€šè·¯æ•¸", store_count)
st.dataframe(summary, use_container_width=True, hide_index=True)

left, right = st.columns(2)
with left:
    st.subheader("å„æ”¯ä»˜åˆ¥äº¤æ˜“é‡‘é¡æ¯”")
    pay_agg = df_f.groupby("æ”¯ä»˜åˆ¥")["é‡‘é¡"].sum().reset_index(name="é‡‘é¡")
    if not pay_agg.empty:
        st.plotly_chart(px.pie(pay_agg, values="é‡‘é¡", names="æ”¯ä»˜åˆ¥"), use_container_width=True)
with right:
    st.subheader("å„é–€å¸‚äº¤æ˜“ç¸½é‡‘é¡æ¯”è¼ƒ")
    store_options = sorted(df_f["åº—å"].unique())
    selected_stores = st.multiselect("é–€å¸‚ç¯©é¸", store_options, default=store_options, key="store_filter")
    df_store = df_f[df_f["åº—å"].isin(selected_stores)].groupby("åº—å")["é‡‘é¡"].sum().sort_values(ascending=True)
    if not df_store.empty:
        st.plotly_chart(px.bar(x=df_store.values, y=df_store.index, orientation="h", labels={"x": "äº¤æ˜“é‡‘é¡", "y": "é–€å¸‚"}), use_container_width=True)

st.subheader("å¸Œåˆ©å‰µæ–°å¨ƒå¨ƒæ©Ÿæ¯æ—¥äº¤æ˜“é‡‘é¡")
daily = df_f.groupby("æ—¥æœŸ")["é‡‘é¡"].sum().reset_index()
st.plotly_chart(px.line(daily, x="æ—¥æœŸ", y="é‡‘é¡"), use_container_width=True)

st.subheader("å„æ”¯ä»˜åˆ¥é‡‘é¡å æ¯”ï¼ˆä¾æ—¥æœŸï¼‰")
pay_daily = df_f.pivot_table(index="æ—¥æœŸ", columns="æ”¯ä»˜åˆ¥", values="é‡‘é¡", aggfunc="sum", fill_value=0)
pay_daily_pct = pay_daily.div(pay_daily.sum(axis=1), axis=0).fillna(0) * 100
pay_daily_pct = pay_daily_pct.reset_index()
fig_stack = go.Figure()
for col in pay_daily_pct.columns:
    if col == "æ—¥æœŸ":
        continue
    fig_stack.add_trace(go.Scatter(x=pay_daily_pct["æ—¥æœŸ"], y=pay_daily_pct[col], name=col, stackgroup="one", mode="lines"))
fig_stack.update_layout(yaxis_title="å æ¯” (%)", xaxis_title="æ—¥æœŸ", hovermode="x unified")
st.plotly_chart(fig_stack, use_container_width=True)

st.subheader("å–®ä¸€é–€å¸‚ç´°éƒ¨")
store_detail = st.selectbox("é¸æ“‡é–€å¸‚", ["ï¼ˆå…¨éƒ¨ï¼‰"] + sorted(df_f["åº—å"].unique()), key="store_detail")
if store_detail != "ï¼ˆå…¨éƒ¨ï¼‰":
    df_s = df_f[df_f["åº—å"] == store_detail]
    r1, r2 = st.columns(2)
    with r1:
        st.markdown(f"**{store_detail} æ¯æ—¥äº¤æ˜“é‡‘é¡**")
        st.plotly_chart(px.line(df_s.groupby("æ—¥æœŸ")["é‡‘é¡"].sum().reset_index(), x="æ—¥æœŸ", y="é‡‘é¡"), use_container_width=True)
    with r2:
        st.markdown(f"**{store_detail} å„æ”¯ä»˜åˆ¥é‡‘é¡**")
        st.plotly_chart(px.bar(df_s.groupby("æ”¯ä»˜åˆ¥")["é‡‘é¡"].sum().reset_index(), x="æ”¯ä»˜åˆ¥", y="é‡‘é¡"), use_container_width=True)
