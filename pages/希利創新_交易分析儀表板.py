"""
å¸Œåˆ©å‰µæ–°å¨ƒå¨ƒæ©Ÿ äº¤æ˜“åˆ†æå„€è¡¨æ¿
è³‡æ–™ä¾†æºï¼šSEGA_TX/å¸Œåˆ©å‰µæ–°/ URS-YYYY-MM-DD.csv
æ¬„ä½å°æ‡‰ï¼šå•†åº—åç¨±â†’åº—åã€äº¤æ˜“æ—¥æœŸâ†’æ—¥æœŸã€ç™¼å¡å…¬å¸â†’æ”¯ä»˜åˆ¥ã€å¯¦éš›æ‰£æ¬¾é‡‘é¡â†’é‡‘é¡
"""
import os
import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime

try:
    from config import BASE_DIR
except ImportError:
    BASE_DIR = os.path.abspath(os.getcwd())

# å¸Œåˆ©å‰µæ–°æ¯æ—¥å ±è¡¨é è¨­ç›®éŒ„ï¼šå˜—è©¦å¤šç¨®ç›¸å°è·¯å¾‘ï¼ˆæœ¬æ©Ÿ Web èˆ‡ SEGA_TX åŒå±¤ï¼›é›²ç«¯å¯èƒ½ç‚º src/Web ä¸” SEGA_TX åœ¨ repo æ ¹ï¼‰
def _resolve_hili_data_dir():
    candidates = [
        os.path.join(BASE_DIR, "..", "SEGA_TX", "å¸Œåˆ©å‰µæ–°"),           # èˆ‡ Web åŒå±¤
        os.path.join(BASE_DIR, "..", "..", "SEGA_TX", "å¸Œåˆ©å‰µæ–°"),     # ä¸Šå…©å±¤ï¼ˆä¾‹å¦‚ /mount/src/Web â†’ /mount/SEGA_TXï¼‰
    ]
    for p in candidates:
        resolved = os.path.abspath(p)
        if os.path.isdir(resolved):
            return resolved
    return os.path.abspath(candidates[0])  # é è¨­é¡¯ç¤ºç¬¬ä¸€å€‹å€™é¸è·¯å¾‘

HILI_DATA_DIR = _resolve_hili_data_dir()

# å ±è¡¨å¿…è¦æ¬„ä½
REQUIRED_COLS = ["å•†åº—åç¨±", "äº¤æ˜“æ—¥æœŸ", "æ˜¯å¦é€€æ¬¾", "ç™¼å¡å…¬å¸", "å¯¦éš›æ‰£æ¬¾é‡‘é¡"]


def _store_short_name(full_name):
    """å¾ã€Œå¸Œåˆ©å‰µæ–°_7-11 æ„¿æ©‹ã€æ“·å–é–€å¸‚ç°¡ç¨±ã€Œæ„¿æ©‹ã€"""
    if pd.isna(full_name) or not isinstance(full_name, str):
        return full_name
    s = full_name.strip()
    if " " in s:
        return s.split()[-1]
    return s


def load_urs_csv(path_or_file, encoding="utf-8"):
    """è¼‰å…¥å–®ä¸€ URS-*.csvï¼ˆè·¯å¾‘æˆ–ä¸Šå‚³æª”æ¡ˆï¼‰ï¼Œå›å‚³æ¨™æº–åŒ– DataFrameï¼ˆåº—åã€äº¤æ˜“æ—¥æœŸã€æ”¯ä»˜åˆ¥ã€é‡‘é¡ï¼‰ã€‚"""
    if hasattr(path_or_file, "read"):
        df = pd.read_csv(path_or_file, encoding=encoding)
    else:
        df = pd.read_csv(path_or_file, encoding=encoding)
    if not all(c in df.columns for c in REQUIRED_COLS):
        return None
    # æ’é™¤é€€æ¬¾
    df = df[df["æ˜¯å¦é€€æ¬¾"].astype(str).str.strip() != "æ˜¯"].copy()
    df["äº¤æ˜“æ—¥æœŸ"] = pd.to_numeric(df["äº¤æ˜“æ—¥æœŸ"], errors="coerce").astype("Int64")
    df = df.dropna(subset=["äº¤æ˜“æ—¥æœŸ"])
    df["æ—¥æœŸ"] = pd.to_datetime(df["äº¤æ˜“æ—¥æœŸ"].astype(str), format="%Y%m%d", errors="coerce")
    df = df.dropna(subset=["æ—¥æœŸ"])
    df["åº—å"] = df["å•†åº—åç¨±"].map(_store_short_name)
    df["æ”¯ä»˜åˆ¥"] = df["ç™¼å¡å…¬å¸"].fillna("ç¾é‡‘").astype(str).str.strip().replace("", "ç¾é‡‘")
    df["é‡‘é¡"] = pd.to_numeric(df["å¯¦éš›æ‰£æ¬¾é‡‘é¡"], errors="coerce").fillna(0)
    return df[["åº—å", "æ—¥æœŸ", "æ”¯ä»˜åˆ¥", "é‡‘é¡", "äº¤æ˜“æ—¥æœŸ"]]


def load_hili_data(data_dir=None, uploaded_files=None):
    """
    è¼‰å…¥å¸Œåˆ©å‰µæ–°äº¤æ˜“è³‡æ–™ã€‚
    - data_dir: ç›®éŒ„è·¯å¾‘ï¼Œè®€å–è©²ç›®éŒ„ä¸‹æ‰€æœ‰ URS-*.csv ä¸¦åˆä½µã€‚
    - uploaded_files: list of UploadedFileï¼Œé€ä¸€è®€å–ä¸¦åˆä½µã€‚
    å›å‚³ (df_raw, error_msg)ã€‚æˆåŠŸæ™‚ error_msg ç‚º Noneã€‚
    """
    if uploaded_files:
        dfs = []
        for f in uploaded_files:
            if f.name.lower().endswith(".csv"):
                try:
                    df = load_urs_csv(f)
                    if df is not None:
                        dfs.append(df)
                except Exception as e:
                    return None, f"è®€å– {f.name} å¤±æ•—ï¼š{e}"
        if not dfs:
            return None, "æ²’æœ‰å¯ç”¨çš„ URS æ ¼å¼ CSVï¼ˆéœ€å«ï¼šå•†åº—åç¨±ã€äº¤æ˜“æ—¥æœŸã€æ˜¯å¦é€€æ¬¾ã€ç™¼å¡å…¬å¸ã€å¯¦éš›æ‰£æ¬¾é‡‘é¡ï¼‰"
        return pd.concat(dfs, ignore_index=True), None

    if not data_dir or not os.path.isdir(data_dir):
        return None, None  # ç„¡è³‡æ–™ã€ç„¡éŒ¯èª¤ï¼ˆå°šæœªé¸æ“‡ä¾†æºï¼‰

    import glob
    pattern = os.path.join(data_dir, "URS-*.csv")
    files = sorted(glob.glob(pattern))
    if not files:
        return None, f"ç›®éŒ„å…§æ²’æœ‰ URS-*.csvï¼š{data_dir}"

    dfs = []
    for path in files:
        try:
            df = load_urs_csv(path)
            if df is not None:
                dfs.append(df)
        except Exception as e:
            return None, f"è®€å– {os.path.basename(path)} å¤±æ•—ï¼š{e}"
    if not dfs:
        return None, "æ²’æœ‰ç¬¦åˆæ¬„ä½æ ¼å¼çš„æª”æ¡ˆ"
    return pd.concat(dfs, ignore_index=True), None


def build_summary_table(df, date_min, date_max):
    """ä¾ç¯©é¸å€é–“æŒ‰åº—åèšåˆï¼šç´¯ç©ç‡Ÿæ”¶ã€å¹³å‡æ—¥ç‡Ÿæ”¶ã€é ä¼°æœˆï¼å¹´ç‡Ÿæ”¶ã€‚"""
    if df.empty:
        return pd.DataFrame()
    days = max(1, (date_max - date_min).days + 1)
    g = df.groupby("åº—å").agg(ç´¯ç©ç‡Ÿæ”¶=("é‡‘é¡", "sum")).reset_index()
    g["å¹³å‡æ—¥ç‡Ÿæ”¶"] = (g["ç´¯ç©ç‡Ÿæ”¶"] / days).round(0)
    g["é ä¼°æœˆç‡Ÿæ”¶"] = (g["å¹³å‡æ—¥ç‡Ÿæ”¶"] * 30).round(0)
    g["é ä¼°å¹´ç‡Ÿæ”¶"] = (g["å¹³å‡æ—¥ç‡Ÿæ”¶"] * 365).round(0)
    cols = ["åº—å", "ç´¯ç©ç‡Ÿæ”¶", "å¹³å‡æ—¥ç‡Ÿæ”¶", "é ä¼°æœˆç‡Ÿæ”¶", "é ä¼°å¹´ç‡Ÿæ”¶"]
    # ç¸½è¨ˆåˆ—
    total_row = {"åº—å": "ç¸½è¨ˆ", "ç´¯ç©ç‡Ÿæ”¶": g["ç´¯ç©ç‡Ÿæ”¶"].sum(), "å¹³å‡æ—¥ç‡Ÿæ”¶": g["å¹³å‡æ—¥ç‡Ÿæ”¶"].sum(),
                 "é ä¼°æœˆç‡Ÿæ”¶": g["é ä¼°æœˆç‡Ÿæ”¶"].sum(), "é ä¼°å¹´ç‡Ÿæ”¶": g["é ä¼°å¹´ç‡Ÿæ”¶"].sum()}
    g = pd.concat([g, pd.DataFrame([total_row])], ignore_index=True)
    # å¹³å‡å€¼ï¼ˆå…¨éƒ¨é–€å¸‚ï¼‰
    store_only = g[g["åº—å"] != "ç¸½è¨ˆ"]
    avg_row = {"åº—å": "å¹³å‡å€¼", "ç´¯ç©ç‡Ÿæ”¶": store_only["ç´¯ç©ç‡Ÿæ”¶"].mean().round(0),
               "å¹³å‡æ—¥ç‡Ÿæ”¶": store_only["å¹³å‡æ—¥ç‡Ÿæ”¶"].mean().round(0),
               "é ä¼°æœˆç‡Ÿæ”¶": store_only["é ä¼°æœˆç‡Ÿæ”¶"].mean().round(0),
               "é ä¼°å¹´ç‡Ÿæ”¶": store_only["é ä¼°å¹´ç‡Ÿæ”¶"].mean().round(0)}
    g = pd.concat([g, pd.DataFrame([avg_row])], ignore_index=True)
    # å¹³å‡å€¼ï¼ˆç‡Ÿæ¥­ä¸­ï¼šç´¯ç©ç‡Ÿæ”¶ > 0ï¼‰
    operating = store_only[store_only["ç´¯ç©ç‡Ÿæ”¶"] > 0]
    if not operating.empty:
        avg_op_row = {"åº—å": "å¹³å‡å€¼ï¼ˆç‡Ÿæ¥­ä¸­ï¼‰", "ç´¯ç©ç‡Ÿæ”¶": operating["ç´¯ç©ç‡Ÿæ”¶"].mean().round(0),
                      "å¹³å‡æ—¥ç‡Ÿæ”¶": operating["å¹³å‡æ—¥ç‡Ÿæ”¶"].mean().round(0),
                      "é ä¼°æœˆç‡Ÿæ”¶": operating["é ä¼°æœˆç‡Ÿæ”¶"].mean().round(0),
                      "é ä¼°å¹´ç‡Ÿæ”¶": operating["é ä¼°å¹´ç‡Ÿæ”¶"].mean().round(0)}
        g = pd.concat([g, pd.DataFrame([avg_op_row])], ignore_index=True)
    return g[cols]


# --- Streamlit é é¢ ---
st.set_page_config(
    page_title="å¸Œåˆ©å‰µæ–° äº¤æ˜“åˆ†æå„€è¡¨æ¿",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded",
)

st.markdown("## å¸Œåˆ©å‰µæ–°å¨ƒå¨ƒæ©Ÿ äº¤æ˜“åˆ†æå„€è¡¨æ¿")

# è³‡æ–™ä¾†æºï¼šå´é‚Šæ¬„
with st.sidebar:
    st.subheader("è³‡æ–™ä¾†æº")
    use_dir = os.path.isdir(HILI_DATA_DIR)
    if use_dir:
        source = st.radio("ä¾†æº", ["å¾ç›®éŒ„è¼‰å…¥ï¼ˆSEGA_TX/å¸Œåˆ©å‰µæ–°ï¼‰", "ä¸Šå‚³ CSV"], key="hili_src")
    else:
        source = "ä¸Šå‚³ CSV"
        st.caption(f"æœªåµæ¸¬åˆ°ç›®éŒ„ï¼š{HILI_DATA_DIR}")

    raw_df = None
    err_msg = None

    if source == "å¾ç›®éŒ„è¼‰å…¥ï¼ˆSEGA_TX/å¸Œåˆ©å‰µæ–°ï¼‰" and use_dir:
        if st.button("è¼‰å…¥ç›®éŒ„å…§æ‰€æœ‰ URS-*.csv"):
            raw_df, err_msg = load_hili_data(data_dir=HILI_DATA_DIR)
    else:
        uploaded = st.file_uploader(
            "ä¸Šå‚³ URS-*.csvï¼ˆå¯å¤šæª”ï¼‰",
            type=["csv"],
            accept_multiple_files=True,
        )
        if uploaded:
            raw_df, err_msg = load_hili_data(uploaded_files=uploaded)

    if err_msg:
        st.error(err_msg)
    if raw_df is not None:
        st.session_state["hili_raw_df"] = raw_df
        st.success(f"å·²è¼‰å…¥ {len(raw_df):,} ç­†æœ‰æ•ˆäº¤æ˜“")

if "hili_raw_df" not in st.session_state or st.session_state["hili_raw_df"] is None:
    st.info("ğŸ‘ˆ è«‹åœ¨å·¦å´é¸æ“‡ã€Œå¾ç›®éŒ„è¼‰å…¥ã€æˆ–ã€Œä¸Šå‚³ CSVã€ä»¥è¼‰å…¥å¸Œåˆ©å‰µæ–°æ¯æ—¥äº¤æ˜“å ±è¡¨ï¼ˆURS-*.csvï¼‰ã€‚")
    st.stop()

df = st.session_state["hili_raw_df"]
date_min_all, date_max_all = df["æ—¥æœŸ"].min(), df["æ—¥æœŸ"].max()

# é ‚éƒ¨ç¯©é¸ï¼šäº¤æ˜“æ—¥æœŸ
st.subheader("äº¤æ˜“æ—¥æœŸç¯©é¸")
col_y1, col_y2, col_m1, col_m2, col_d1, col_d2 = st.columns(6)
with col_y1:
    y1 = st.number_input("èµ·å§‹å¹´", min_value=date_min_all.year, max_value=date_max_all.year, value=date_min_all.year, key="y1")
with col_m1:
    m1 = st.number_input("èµ·å§‹æœˆ", min_value=1, max_value=12, value=date_min_all.month, key="m1")
with col_d1:
    d1 = st.number_input("èµ·å§‹æ—¥", min_value=1, max_value=31, value=date_min_all.day, key="d1")
with col_y2:
    y2 = st.number_input("çµæŸå¹´", min_value=date_min_all.year, max_value=date_max_all.year, value=date_max_all.year, key="y2")
with col_m2:
    m2 = st.number_input("çµæŸæœˆ", min_value=1, max_value=12, value=date_max_all.month, key="m2")
with col_d2:
    d2 = st.number_input("çµæŸæ—¥", min_value=1, max_value=31, value=date_max_all.day, key="d2")

try:
    filter_start = datetime(y1, m1, d1).date()
    filter_end = datetime(y2, m2, d2).date()
except ValueError:
    filter_start = date_min_all.date()
    filter_end = date_max_all.date()

df_f = df[(df["æ—¥æœŸ"].dt.date >= filter_start) & (df["æ—¥æœŸ"].dt.date <= filter_end)].copy()
if df_f.empty:
    st.warning("ç¯©é¸å€é–“å…§ç„¡è³‡æ–™ï¼Œè«‹èª¿æ•´æ—¥æœŸã€‚")
    st.stop()

stat_days = max(1, (filter_end - filter_start).days + 1)
last_date = df_f["æ—¥æœŸ"].max().strftime("%Y-%m-%d")
store_count = df_f["åº—å"].nunique()

# è³‡æ–™é‡æ•´è¡¨
summary = build_summary_table(df_f, pd.Timestamp(filter_start), pd.Timestamp(filter_end))
st.subheader("è³‡æ–™é‡æ•´")
c1, c2, c3 = st.columns(3)
c1.metric("æœ€å¾Œäº¤æ˜“æ—¥", last_date)
c2.metric("çµ±è¨ˆæ—¥æ•¸", stat_days)
c3.metric("ç‡Ÿé‹é€šè·¯æ•¸", store_count)
st.dataframe(summary, use_container_width=True, hide_index=True)

# åœ–è¡¨å€ï¼šå…©æ¬„
left, right = st.columns(2)

with left:
    st.subheader("å„æ”¯ä»˜åˆ¥äº¤æ˜“é‡‘é¡æ¯”")
    pay_agg = df_f.groupby("æ”¯ä»˜åˆ¥")["é‡‘é¡"].sum().reset_index(name="é‡‘é¡")
    if not pay_agg.empty:
        fig_pie = px.pie(pay_agg, values="é‡‘é¡", names="æ”¯ä»˜åˆ¥", title="")
        st.plotly_chart(fig_pie, use_container_width=True)

with right:
    st.subheader("å„é–€å¸‚äº¤æ˜“ç¸½é‡‘é¡æ¯”è¼ƒ")
    store_options = sorted(df_f["åº—å"].unique())
    selected_stores = st.multiselect("é–€å¸‚ç¯©é¸", store_options, default=store_options, key="store_filter")
    df_store = df_f[df_f["åº—å"].isin(selected_stores)].groupby("åº—å")["é‡‘é¡"].sum().sort_values(ascending=True)
    if not df_store.empty:
        fig_bar = px.bar(x=df_store.values, y=df_store.index, orientation="h", labels={"x": "äº¤æ˜“é‡‘é¡", "y": "é–€å¸‚"})
        st.plotly_chart(fig_bar, use_container_width=True)

# æ¯æ—¥äº¤æ˜“é‡‘é¡æŠ˜ç·šåœ–
st.subheader("å¸Œåˆ©å‰µæ–°å¨ƒå¨ƒæ©Ÿæ¯æ—¥äº¤æ˜“é‡‘é¡")
daily = df_f.groupby("æ—¥æœŸ")["é‡‘é¡"].sum().reset_index()
fig_line = px.line(daily, x="æ—¥æœŸ", y="é‡‘é¡", title="")
st.plotly_chart(fig_line, use_container_width=True)

# å„æ”¯ä»˜åˆ¥é‡‘é¡ï¼ˆæ™‚é–“ï¼‰ç™¾åˆ†æ¯”å †ç–Š
st.subheader("å„æ”¯ä»˜åˆ¥é‡‘é¡å æ¯”ï¼ˆä¾æ—¥æœŸï¼‰")
pay_daily = df_f.pivot_table(index="æ—¥æœŸ", columns="æ”¯ä»˜åˆ¥", values="é‡‘é¡", aggfunc="sum", fill_value=0)
pay_daily_pct = pay_daily.div(pay_daily.sum(axis=1), axis=0).fillna(0) * 100
pay_daily_pct = pay_daily_pct.reset_index()
fig_stack = go.Figure()
for col in pay_daily_pct.columns:
    if col == "æ—¥æœŸ":
        continue
    fig_stack.add_trace(go.Scatter(
        x=pay_daily_pct["æ—¥æœŸ"], y=pay_daily_pct[col], name=col, stackgroup="one", mode="lines"
    ))
fig_stack.update_layout(yaxis_title="å æ¯” (%)", xaxis_title="æ—¥æœŸ", hovermode="x unified")
st.plotly_chart(fig_stack, use_container_width=True)

# å–®ä¸€é–€å¸‚ç´°éƒ¨
st.subheader("å–®ä¸€é–€å¸‚ç´°éƒ¨")
store_detail = st.selectbox("é¸æ“‡é–€å¸‚", ["ï¼ˆå…¨éƒ¨ï¼‰"] + sorted(df_f["åº—å"].unique()), key="store_detail")
if store_detail != "ï¼ˆå…¨éƒ¨ï¼‰":
    df_s = df_f[df_f["åº—å"] == store_detail]
    r1, r2 = st.columns(2)
    with r1:
        st.markdown(f"**{store_detail} ä¹‹äº¤æ˜“é‡‘é¡ï¼ˆæ¯æ—¥ï¼‰**")
        daily_s = df_s.groupby("æ—¥æœŸ")["é‡‘é¡"].sum().reset_index()
        fig_s_line = px.line(daily_s, x="æ—¥æœŸ", y="é‡‘é¡", title="")
        st.plotly_chart(fig_s_line, use_container_width=True)
    with r2:
        st.markdown(f"**{store_detail} å„æ”¯ä»˜åˆ¥é‡‘é¡**")
        pay_s = df_s.groupby("æ”¯ä»˜åˆ¥")["é‡‘é¡"].sum().reset_index()
        fig_s_bar = px.bar(pay_s, x="æ”¯ä»˜åˆ¥", y="é‡‘é¡", title="")
        st.plotly_chart(fig_s_bar, use_container_width=True)
